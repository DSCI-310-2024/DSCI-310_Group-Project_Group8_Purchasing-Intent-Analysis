---
title: "DSCI 310 Group 8: Predicting Online Shopper Purchasing Intention"
author: "Calvin Choi, Nour Abdelfattah, Sai Pusuluri, Sana Shams"
format: 
    html:
        toc: true
        toc-depth: 2
        embed-resources: true
bibliography: references.bib
execute: 
  echo: false
  warning: false
editor: source 
---

```{python}
import pandas as pd
from IPython.display import Markdown, display
from tabulate import tabulate
```

## Abstract 
**Question**: Using all variables provided in the dataset, which group of explanatory variables form the best prediction for the user's purchase intent?

E-commerce pages host several customers at any given moment, yet its metric of success lies in the visitors who ultimately make purchase. This project uses several machine learning models to learn from webpage data and customer browsing behaviour in order to predict whether or not a given customer will finalize their purchase.

 **Findings** 

To use *all variables* that were explanatory for our target feature, Revenue, it was found that RandomForestClassifier performed the best when it came to fit time, score time, and overall prediction score. 

From the Random Forests model, it was found that the following group of explanatory variables form the best prediction for the user's purchase intent:
* PageValues
* ProductRelated_Duration 
* ExitRates
* ProductRelated
* Administrative_Duration

**Limitations**

When examining metrics such as precision, accuracy, recall and f1 score, the values were somewhat low, indicating that the random forest model's performance on the test set was not as good as its performance on the training set. This is indicative of the issue of class balance, as well as potential overfitting on the training set leading to poor generalization on unseen data.

## Introduction 
It's been no surprise that retail giants like Walmart and Ikea have aggressively invested and developed their e-commerce experiences transitioning away from big box store fronts and converting those assets to hubs for location-based fulfillment @Monteros_2023. The post-pandemic affects on consumer behaviour have accelerated our dependency on digital platforms and have pushed the e-commerce industry to grow a whopping 25% to an industry worth over $4 trillion USD @Shaw_Eschenbrenner_Baier_2022. Consequently, online storefronts get a lot of site traffic but what ultimately matters is their decision to purchase and the volume of revenue. Marketing and User Experience teams are tasked with optimizing a site’s interface and content in order to improve customer retention and the site’s revenue. Given this, understanding customer browsing behaviour and web page features is crucial for not only improving the user’s experience, but also maximizing the retailer’s revenue. Traditionally marketing and user experience studies are conducted through surveys, interviews and ethnographic studies, taking weeks up to months to process. However, machine learning-based marketing research has exponentially reduced the rate at which web metrics and purchase conversion strategies can be processed, while significantly increasing purchase prediction accuracy @Gkikas2022. A common method to evaluate user retention for online web browsing is through clickstream data of the user’s navigation path, however Saka et al. found that combining this information with session information significantly improves the purchase success rate @Sakar_Polat_Katircioglu_Kastro_2018. 


This project aims to analyze various features of online shopper’s sessions on a site to predict whether the customer makes a purchase. 

**Data**

The Data being used is sourced from the UCI Machine Learning Repository: [Online Shoppers Purchasing Intention](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset). The data is provided from the study, "Real-time prediction of online shoppers’ purchasing intention using multilayer perceptron and LSTM recurrent neural networks" by @Sakar_Polat_Katircioglu_Kastro_2018. This dataset was chosen specifically due to its coverage of both user navigation data and session information, allowing for a well-rounded analysis of both the user and e-commerce page's profile. The ID for this dataset under the UCI Machine Learning repository is: 468

**Question**

Using all variables provided in the dataset, which group of explanatory variables form the best prediction for the user's purchase intent?

Breaking down the question:
* "All variables": to not eliminate potentially useful variables, we will use various models that will look at all the available variables and use the best performing model to select the most important/influential variables.

* "Prediction for the user's purchase intent": User purchase intent is described by the variable *Revenue*, which is a True/False variable. 'True' refers to successful user purchase, and 'False' refers to user did not finalize purchase. The models we explore will use this as the response variable, and that is the predictor variable for our final model, which will be applied to the test set.

* "Explanatory variables": this refers to all variables except for "Revenue"


## Exploratory Data Analysis
Before starting our analysis, we will perform exploratory data analysis in order to have a better understanding of the distributions of the features in our dataset, as well as their contribution to our target feature, Revenue. Exploratory Data Analysis will only be conducted on the train set in order to ensure that the Golden Rule (must not expose test data during preliminary data analysis and training of any models) of Machine Learning is not broken.

The dataset provides the following features:

#### Summary of features

* **Administrative**: the number of pages visited by user of this administrative type
* **Administrative_Duration**: the amount of time spent on pages of this administrative type
* **Informational**: the number of pages visited by user of this informational type
* **Informational_Duration**: the amount of time spent on this informational category of pages
* **ProductRelated**: the number of pages of this type of product the user visited
* **ProductRelated_Duration**: the amount of time spent on pages featuring related products
* **BounceRates**: percentage of visitors who enter the web page then leave ("bounce") without triggering any other requests to the analytics server during the session
* **ExitRates**: the percentage of pageviews where the given page is the last page before exiting website
* **PageValues**: the average value for a web page that a user visited before completing an e-commerce transaction
* **SpecialDay**: the temporal proximity between the day the user is visiting the page and a special day (eg. Valentines Day, Christmas, Mother's Day, etc.). 
* **Month**: the month the page was viewed
* **OperatingSystems**: an integer value representing the operating system of the user when viewing the page
* **Browser**: an integer value representing the user's browser when viewing the page
* **Region**: an integer value representing the user's traffic type. [Learn more about user traffic types here](https://www.practicalecommerce.com/Understanding-Traffic-Sources-in-Google-Analytics)
* **VisitorType**: categorizes the user into 'New Visitor', 'Returning Visitor', and 'Other'.
* **Weekend**: a boolean value, indicating whether the user's session took place during a weekend or not
* **Revenue**: a boolean value, indicating whether the user made the purchase or not, where 0=False and 1=True
    * **This will be our target feature**

**Note**
Unfortunatly the UCI ML repository not the paper that sourced the data (@Sakar_Polat_Katircioglu_Kastro_2018) provide interpretations for the number-system classification for the following variables: Browser, Region, OperatingSystems. 


#### Exploratory Visualization
To inform the chosen method of visualization, let us first document if the features are continuous values, or if they are discrete categorical values. Some features are categorical but represented as integers so this step will allow for clarification. 


| Feature                  | Type                          |
|--------------------------|-------------------------------|
| Administrative           | numerical, continuous         |
| Administrative_Duration  | numerical, continuous         |
| Informational            | numerical, continuous         |
| Informational_Duration   | numerical, continuous         |
| ProductRelated           | numerical, continuous         |
| ProductRelated_Duration  | numerical, continuous         |
| BounceRates              | numerical, continuous         |
| ExitRates                | numerical, continuous         |
| PageValues               | numerical, continuous         |
| SpecialDay               | numerical, continuous         |
| Month                    | categorical, discrete         |
| Browser                  | categorical, discrete         |
| Region                   | categorical, discrete         |
| TrafficType              | categorical, discrete         |
| VisitorType              | categorical, discrete         |
| Weekend                  | categorical, discrete, boolean|
| Revenue                  | categorical, discrete, boolean (0=False, 1=True)|
: Data Types Summary {#tbl-features}

The summary of each datatype is provided in @tbl-features 

Given that Revenue is our target feature, let us examine its class distribution in @fig-revenue-distribution.

![Revenue Class Distribution](../img/eda__revenue_class_distribution.png){#fig-revenue-distribution width=80%} 

@fig-revenue-distribution shows that there does seem to be some class imbalance in the Revenue feature, where the False(0) cases outnumber the True(1) cases by more than double the amount. This might create bias in our models that may perform poorly on the 'True' Revenue cases as there were less examples to fit on.

To compare Revenue with other features, we will perform some feature engineering by creating the feature, Total Revenue, for each of the categorical revenues explored below.

#### Categorical Features: Examining the Distributions

Let us examine the distribution of certain categorical features to better understand the user demographic.The distributions are:

* distribution of classes within the categorical feature

* distribution of Revenue=True(1) across the different classes for a given feature

![Distribution of Sessions by Month](../img/eda__month_distribution.png){#fig-month-distribution width=80%}

@fig-month-distribution shows that most of the sessions occur during the months May, November, March, and December while successful purchases occur mostly in the months of May, November and December. 

![Distribution of Sessions by Browser](../img/eda__browser_distribution.png){#fig-browser-distribution width=80%}

@fig-browser-distribution shows that the majority of sessions are hosted on Browser class 2 and 1, whith the most number of purchases being made in browser class 2 and 1. 

![Distribution of Sessions by Region](../img/eda__region_distribution.png){#fig-region-distribution width=80%}

@fig-region-distribution shows that the majority of sessions are hosted in regions 2, 1, and 3, which are also the regions with the most number of purchases.

![Distribution of Sessions by Visitor Type](../img/eda__visitor_type_distribution.png){#fig-visitor-distribution width=80%}

@fig-visitor-distribution shows that the majority of sessions are from Returning visitors, which also represent the majority of purchases. This indicates that a Revenue = True is more likely when the user accessing the session has already visited and viewed the product, compared to first-time visitors.

![Distribution of Sessions by Weekend](../img/eda__weekend_distribution.png){#fig-weekend-distribution width=80%}

@fig-weekend-distribution shows that the majority of sessions and the majority of purchases occur on weekdays as opposed to weekends. 

#### Continuous numerical Features: Correlation with Revenue

The remaining features are continuous numerical features. In order to understand their significance to the target feature, Revenue, we have made a correlation plot representing all numerical features, as well as Revenue which was converted to a numerical format during data cleaning.

![Correlation Matrix of Numerical Features](../img/eda__correlation_matrix.png){#fig-corr_matrix width=80%}

From @fig-corr_matrix, we can see that the features most strongly correlated with Revenue are: PageValues, ProductRelated, and ProductRelated_Duration. It is important to note that the correlation matrix only represents linear correlations, between *pairs* of features. Some correlations may be confounded with other features, so while this matrix is a good starting point, it may not capture all relevant relationships.

## Methods


### Analysis Plan

## Methods

### Plan Summary
Given that we want to examine all explanatory variables, we will first examine the performance of several models to see which performs the best in predicting the target feature in the training set. Once that model is obtained, we obtain feature importances to gauge the explanatory variables' importance when predicting the target variable.

#### 1. Train-Test Split

A 70-30 split of our data has already been applied after loading and cleaning the data, before performing any EDA: 

* 70% split for the training subset

* 30% split for the testing subset



#### 2. Preprocessing and Transformations

Given that we have different data types, we will apply some transformations to each feature type depending on if the feature is numerical and continuous, discrete and categorical, binary, etc. The transformations will be detailed in Figure 9. 

#### 3. Training Models

Our target feature, Revenue, is binary so the models chosen will be trained to perform binary classification:

**3.1 Dummy Classifier Model:**

The Dummy Classifier Model makes predictions that ignore the input features, in other words, it does not attempt to 'learn' anything from the data. This classifier serves as a baseline to compare to the following models. 

**3.2 kNN:** 

kNN is a simple cluster-based model. Given k, the number of nearest data points, the kNN classifier takes a data point and classifies it according to the the class of its k-nearest neighbours.


**3.3 SVM RBF:** 

Support Vector Machines with RBF Kernels act as weighted KNN's. Unlike KNN's, this model bases its decision boundary only on key examples, known as support vectors. The model transforms the input features into a higher dimensional space, generating a decision boundary based on a set of positive and negative examples and their weights along with their similarity measure. This model uses a kernel called RBFs as the similarity metric. 

**3.4 Random Forest Classifier:**

A Random Forest Classifier fits a series of decision tree classifiers on subsets of the given data. Each tree 'overfits' on a select feature, however the model uses averaging of individual trees to improve the predictive accuracy and therefore prevent overfitting. Given that there are many features in our dataset, this model is a strong candidate for our classification problem.


##### Evaluating the models
Each model will be evaluated on the following:

* fit time

* score time

* test score (this is the validation score, not the score from the actual test subset)

* train score

The model with the best (validation) test score will then be deployed **once** on the test data to obtain a final test score.


### Analysis

### 1. Train Test Split

**Note**: 

All training of the models will be strictly conducted on the training set. The testing set will only be used once the model is finalized, and will only be deployed for scoring on the testing set once. This is to ensure that the model is not exposed to the testing set so that it does not 'learn' off of what it is trying to predict.


### 2. Preprocessing and Transformations


| **Feature** | **Transformation** | **Explanation**|
|-------------|--------------------|----------------|
| Administrative | scaling |standardize scale with other numerical features|
| Administrative_Duration | scaling |standardize scale with other numerical features |
| Informational | scaling |standardize scale with other numerical features |
| Informational_Duration | scaling |standardize scale with other numerical features |
| ProductRelated | scaling |standardize scale with other numerical features|
| ProductRelated_Duration | scaling |standardize scale with other numerical features |
| BounceRates | scaling |standardize scale with other numerical features|
| ExitRates | scaling |standardize scale with other numerical features|
| PageValues | scaling |standardize scale with other numerical features|
| SpecialDay | scaling |standardize scale with other numerical features |
| Month | one-hot encoding |categorical feature, need a numerical representation to pass through models|
| OperatingSystems | drop | justified in EDA, not relevant |
| Browser | n/a | would apply one-hot encoding but already represented in numerical form |
| Region | n/a | would apply one-hot encoding but already represented in numerical form |
| TrafficType | n/a | would apply one-hot encoding but already represented in numerical form |
| VisitorType | one-hot encoding | categorical feature, need a numerical representation to pass through models|
| Weekend | one-hot encoding with 'binary=True' | categorical feature, need a numerical representation to pass through models|
: Data Preprocessing Summary {#tbl-preprocessing}

The summary of preprocessing transformations and the explanation behind each is provided in @tbl-preprocessing. 

### Training Models

For this analysis, we defined a custom function that returns the mean and standard deviation cross validation scores. For each model, a pipeline was defined to first preprocess the training split of the data, then pass it through the model to be fit. The documentation of the function is as follows:

**Parameters**

* model :scikit-learn model

* X_train : numpy array or pandas DataFrame

    * X in the training data

* y_train :

    * y in the training data

**Returns**

* pandas Series with mean scores from cross_validation

**Note**: this function definition is taken from CPSC330 2023W1 Course Notes

Moreover, the results of each of the models described in the Analysis Plan section is stored in a table to facilitate comparison of their fit time, score time, test score, and train scores -- including the cross validation scores for each metric.

#### Model Results 

```{python}
#| label: tbl-model-results
#| tbl-cap: the fit time, score time, test score, and train score for all models evaluated

# Load the CSV file into a pandas DataFrame
models_df = pd.read_csv("../results/model_comparison_results.csv")

# Display the DataFrame
Markdown(models_df.to_markdown(index = False))

```

### Model Selection 
Among the many factors that data scientists consider for model selection, the two main factors we will consider are performance and efficiency. Let us consider these factors given the results displayed in @tbl-model-results

#### Efficiency

Time efficiency and computational complexity are key factors to model selection. We want models that strike the best balance between getting the results we want and doing it in an efficient manner. KNN is known for its computational intensity, although not reflected in the fit times above, we can be concerned how it might performed on a larger scale. Similarly, we know that the SVM model struggles with computational efficiency and that is what our results reflected in the the fit time. The random forests model came out on top of the efficiency tables with the quickest fit time and scoring time while outperforming all other models in terms of score as well.

#### Performance

In assessing the accuracy of machine learning models, particularly in the context of comparing Random Forests, kNN, and SVM, it's essential to evaluate various metrics and considerations that provide an in-depth evaluation truly reflective of performance. Each of these models offers distinct approaches to classification and regression tasks, leading to variations in performance across different datasets and problem domains. In terms of our results, the clear winner is the random forest model. 

Given that each model takes in all the features in the dataset, these findings help to address the following part of our question: **Using all variables provided in the dataset**, which group of explanatory variables form the best prediction for the user's purchase intent?

#### Determining Feature Importance
Now that we have a model that best learns and predicts on our training set, let us determine which features play the greatest importance. 

```{python, echo: false}
#| label: tbl-feature-importance
#| tbl-cap: Feature importance scores for all variables in the random forests model


# Load the CSV file into a pandas DataFrame
feature_importance = pd.read_csv("../results/feature_importances.csv")

# Display the DataFrame
feature_importance

```

From @tbl-feature-importance, we can see ranked importance scores, which indicate how much each feature is contributing to the Radnom Forests model. For categorical models which have been one-hot encoded, such as Month, the label is Month_[insert mont] for example Month_June. The values in this column represent the relative importance of each feature, where a higher score indicates more importance. From this ranking, we can infer that the following features had high relative importance:

* PageValues

* ProductRelated_Duration 

* ExitRates

* ProductRelated

* Administrative_Duration


The rest of the variables gradually decrease, however it is worth nothing that the variable PageValues is significantly higher than all other features (by approximately 4 times), implying this is one of the most influential features when it comes to predicting user purchasing intent (represented by variable Revenue).

These results are actually on par with the numerical features with the highest correlations to Revenue, as observed during EDA in @fig-corr_matrix. 

These findings address the following part of our question: Using all variables provided in the dataset, **which group of explanatory variables form the best prediction for the user's purchase intent**?

#### Evaluating the chosen model on our test data

To evaluate the performance of our chosen model, random forests, on the test data, we will examine the following metrics: Precision, Recall, Accuracy, and F1 Score.

**Breakdwon of Metrics**

* **Precision**: the ratio of correctly predicted positive cases to the total number of predicted positive cases (true positives and false positives)

* **Recall**: the ratio of correctly predicted positive observations to the total number of positive cases (true positives and false negatives)

* **Accuracy**: the ratio of correctly predicted observations to the total number of observations. This metric may be misleading if classes are imbalanced

* **F1 Score**: the weighted average of precision and recall, this is a better measure for imbalanced classes compared to accuracy

These scores were obtained using True Negative, True Positive, Fasle Negative, False Negative scores obtained by the confusion matrix of our random forests model, @fig-confusion-matrix.

![Random Forests Confusion Matrix](../results/random_forest_confusion_matrix.png){#fig-confusion-matrix width=80%}


```{python, echo: false}
#| label: tbl-metrics
#| tbl-cap: Performance of the random forest classifier on the test data 

# Load the CSV file into a pandas DataFrame
performance = pd.read_csv("../results/random_forest_metrics.csv")

# Display the DataFrame
performance

```

From the results displayed in @tbl-metrics, our f1 score is somewhat low, 63.28%. Given this, we can infer that the performance of our random forests model -- while not bad -- is affected by class imbalance. This is a limitation of our analysis.

If we break down further our results by looking at our true positive rate (precision score), we can see that our model actually struggled and because of the class imbalance the effect of this on our score was not reflected in our accuracy. This analysis helps us tame our optimism and helps us insights into how well our model correctly identifies positive instances within the dataset, which is particularly crucial in scenarios with imbalanced classes. Understanding the nuances of class imbalances allows us to make informed decisions about model adjustments, such as implementing techniques like oversampling, undersampling, or adjusting class weights, to improve the model's predictive capability for minority classes. Moreover, it underscores the importance of considering multiple evaluation metrics beyond just accuracy, enabling a more comprehensive assessment of model effectiveness and guiding future iterations or refinements to enhance predictive performance across all classes.

#### Conclusion and Discussion

Restating the question: 

**Question**: Using all variables provided in the dataset, which group of explanatory variables form the best prediction for the user's purchase intent?

Our findings in regard to the question be summed up into 2 main points:

- When it comes to the application of ML to predict online consumer behaviour given all the explanatory variables in our dataset, the random forests model performs the best and produced the best results in terms of time and train/test scoring. The random forests model performed the best in terms of fit/score time and train/test score out of: a dummy model, SVM RBF model, kNN model and Random Forest Classifier.

- The following group of explanatory variables have the greatest feature importance in our chosen model when predicting the target feature, Revenue:
1. PageValues
2. ProductRelated_Duration 
3. ExitRates
4. ProductRelated
5. Administrative_Duration

##### Limitations
Our results are purely based on the models that we chose to test, and therefore, may not be the absolute best off the shelf models for the given project but in favour of simplicity and the essence of time, among the 3 tested models (KNN, SVM and RF), we've concluded that the random forests model performed the best.

Once our best performing model, random forests, was fitted on the test data, we obtained the following peformance metrics: precision, accuracy, recall and f1 score. We decided to focus on f1 score, as we know from EDA that our dataset has class imbalance, and f1 score encompasses both precision and recall while being a better metric than accuracy when there is class imbalance. It was found that our f1 score was somewhat low, indicating that the random forest model's performance on the test set was not as good as its performance on the training set. This could be indicative that the model was quite overfitted on the training model, or that despite the randomization of our train-test split, the training sample was not a good representation of the test split and perhaps the true population of online shoppers sampled in our dataset. Likely, the low precision score is highly influenced by the fact that there was class imbalance in our dataset, as demonstrated in @fig-revenue-distribution, which shows that there are over 4 times more false cases than true for the feature Revenue. As a consequence, the models, no matter how well they perform on the training dataset, are bound to better learn the predictive features for a false Revenue case than a true revenue case simply because of the class distribution in the original dataset. 

For future analysis, perhaps the class imbalance may be addressed by selectively sampling to have fair distribution of both Revenue classes, rather than pure random sampling as was done in our analysis.

## References

